# Data Collection

This directory contains all scripts and documentation for **replicating the data collection process**.

## üéØ Purpose

Enable other research groups to:
1. Extract the same EPO dataset
2. Understand data collection methodology
3. Replicate and extend the scraping process

## üìä Data Sources

### 1. EPO Deep Tech Finder (Primary Source)

**URL**: https://dtf.epo.org/datav/public/dashboard-frontend/host_epoorg.html

**Data Coverage**:
- 11,270+ European deeptech entities
- 39 European countries
- Patent data from 1978-2025

**Collection Method**: Browser-based extraction (API is Cloudflare-protected)

**Extraction Methods** (try in order):

#### Method 1: Automated Multi-Method Script (Recommended)
```bash
# Install dependencies
pip3 install playwright cloudscraper
python3 -m playwright install chromium

# Run automated extraction (tries multiple methods)
python3 data_collection/extract_epo_all_methods.py
```
This script automatically tries:
- Playwright with real user interactions (clicks, scrolls, waits)
- Cloudscraper (Cloudflare bypass library)
- Falls back to manual instructions if needed

#### Method 2: Playwright Only (Best for Cloudflare)
```bash
pip3 install playwright
python3 -m playwright install chromium
python3 data_collection/extract_epo_playwright.py
```
Uses real browser with stealth plugins, actually clicks around like a human.

#### Method 3: Cloudscraper Only (Lightweight)
```bash
pip3 install cloudscraper
python3 data_collection/extract_epo_cloudscraper.py
```
Python library specifically designed to bypass Cloudflare.

#### Method 4: Manual Browser Console (Fallback)
1. Open `EXTRACT_NOW.html` in your browser
2. Follow the instructions
3. Run the extraction script in browser console
4. Data downloads automatically (~15-20 MB JSON file)
5. Move to `/research/data/raw/`

**Estimated Time**: 
- Automated: 10-20 minutes (depending on method)
- Manual: 5-10 minutes  
**Last Extracted**: [To be filled when you run it]

### Data Schema

Each entity contains (raw extract; **field names as returned by the `applicants` list API**):
```json
{
  "unique_ID": "string - Unique identifier",
  "name": "string - Entity name",
  "role": "string - company|school|pro",
  "country_name": "string - Country name",
  "city": "string|null - City",
  "latitude": "number - Geo coordinate",
  "longitude": "number - Geo coordinate", 
  "homepageUrl": "string|null - Homepage URL (sometimes missing scheme)",
  "tagline": "string|null - Tagline",
  "totalPatents": "integer - Total patent applications (count)",
  "totalGrantedPatents": "integer - Total granted patents (count)",
  "investors": "array[object] - Investor/spinout relations (may be empty)",
  "spinoutsOfUniversity": "array|null - Links to university spinouts (often empty/null)",
  "spinoutsOfPRO": "array|null - Links to PRO spinouts (often null)",
  "company_info": "object - Company metadata (mostly null fields)",
  "school_info": "object - School metadata (mostly null fields)",
  "pro_info": "object - PRO metadata (mostly null fields)"
}
```

**Processed outputs** (generated by `research/analysis/01_data_processing.py`) live in `research/data/processed/` and standardize field names (e.g. `id`, `type`, `country`) and add derived variables.

### Schema completeness vs the EPO website

The EPO website can (in principle) use **additional endpoints** beyond the `applicants` list endpoint. To ensure we are not missing variables that the UI exposes elsewhere, run:

```bash
python3 data_collection/audit_epo_site_schema_playwright.py
```

This produces a report under `research/data/raw/epo_schema_audit_YYYY-MM-DD.json` listing:
- All API endpoints observed during a real browser session
- All API endpoints referenced in the frontend JS bundle(s)
- A comparison between the live `applicants` schema and our latest saved extract

If new endpoints/fields are discovered, we should extend extraction to collect those payloads as well (typically a second-stage, per-entity detail fetch).

To specifically address the ‚Äúper-entity detail‚Äù concern (e.g., the **Funding history** view and the **European patent applications** table shown in the UI), run the deep audit:

```bash
python3 data_collection/audit_epo_site_schema_playwright.py --deep
```

This attempts to click into those views and capture any additional API endpoints used there.

### 2. EPO Publications (Per-Entity Patent Details)

**What it is**: Individual patent records (not just counts). The "European patent applications" table you see in the EPO UI for each entity.

**Why we need it**: The `applicants` endpoint only gives `totalPatents=17` (a count). The `publications` endpoint gives you the **17 actual patents** with:
- Patent titles (technology descriptions)
- Technical field labels (e.g., "Oncology", "Energy", "Computer hardware")
- Filing dates and years
- Grant status ("EP granted", "Pending", "Refused / Withdrawn")
- Co-applicants (collaboration networks)

**Endpoint**: `POST /datav/public/datavisualisation/api/dataset/1/publications`  
**Payload**: `{"filters": [{"filter_id": "org_id", "filter_values": [{"id": "FR0283"}]}], "nextPageToken": ""}`

**How to extract**:
```bash
# Test with 50 entities
python3 data_collection/extract_epo_publications_playwright.py --limit 50

# Full extraction (all 11,270 entities, takes 3-5 hours)
python3 data_collection/extract_epo_publications_playwright.py --resume
```

**Output**: `research/data/raw/epo_publications_YYYY-MM-DD.jsonl` (one JSON object per entity, each with a `publications` array)

**Processing**: 
```bash
python3 research/analysis/02_publications_processing.py
```
Creates `research/data/processed/publications.csv` with one row per patent for analysis.

## üåê Website enrichment (public web)

Once `research/data/processed/companies.csv` exists, you can fetch each company homepage and (optionally) a few internal pages to capture enough *raw* website text for downstream product-positioning extraction:

```bash
python3 data_collection/enrich_websites.py --limit 50
```

Outputs JSONL to `research/data/enriched/websites_raw_YYYY-MM-DD.jsonl` (one record per company with a `pages` array + `combined_text`).

## üíº Dealroom enrichment (import + merge)

We avoid credentialed scraping of Dealroom in this repo. Instead, export a CSV from Dealroom (according to your access/ToS) and merge it onto our EPO companies table:

```bash
python3 data_collection/import_dealroom_export.py --dealroom /path/to/dealroom_export.csv
```

Outputs `research/data/enriched/companies_with_dealroom_YYYY-MM-DD.csv`.

## üîÑ Replication Protocol

### Requirements
- Modern web browser (Chrome, Firefox, Safari, Edge)
- Internet connection
- 20 MB free disk space

### Step-by-Step

1. **Extract EPO Data**
   ```bash
   # Open the extraction tool
   open data_collection/EXTRACT_NOW.html
   
   # Follow on-screen instructions
   # File downloads automatically to Downloads/
   
   # Move to project
   mv ~/Downloads/epo_deeptech_complete_*.json research/data/raw/
   ```

2. **Verify Extraction**
   ```bash
   python3 -c "import json,glob; p=sorted(glob.glob('research/data/raw/epo_deeptech_complete_*.json'))[-1]; d=json.load(open(p)); print('file:',p); print('Extracted',len(d['entities']),'entities')"
   ```

3. **Process Data** (see `/research/analysis/` for scripts)

### Troubleshooting

**Problem**: Script errors in browser console  
**Solution**: Ensure you're on the correct EPO page before pasting script

**Problem**: Incomplete extraction (< 10,000 entities)  
**Solution**: Check browser console for errors; EPO may have rate-limited. Wait 5 minutes and try again.

**Problem**: No download  
**Solution**: Check browser's download settings; some browsers block automatic downloads

## üìù Data Collection Notes

### Extraction Methodology

We use multiple approaches to bypass Cloudflare's anti-bot protection:

1. **Playwright with Real Interactions**: Launches a real browser (non-headless), uses stealth plugins to hide automation, and actually clicks around the UI like a human would. This establishes a legitimate session before making API calls.

2. **Cloudscraper**: Python library specifically designed to bypass Cloudflare by handling JavaScript challenges and mimicking browser behavior.

3. **Browser Console Extraction**: Uses the browser's native `fetch()` API from within an active session, bypassing Cloudflare protection that blocks Python/curl requests.

4. **Pagination**: The API uses `nextPageToken` for pagination. We iterate through all pages until no token is returned.

5. **Rate Limiting**: 300-800ms random delays between requests to mimic human behavior and be respectful to EPO servers.

6. **Data Completeness**: We extract ALL available entities in the EPO database at time of extraction.

### Known Limitations

1. **Dynamic Data**: EPO updates their database periodically. Re-run extraction quarterly for fresh data.

2. **Incomplete Records**: Some entities have missing fields (e.g., no funding data, no parent institution).

3. **Patent Status**: Patent statuses may be outdated by several months.

4. **No Historical Data**: Extraction is point-in-time; no historical snapshots.

### Future Data Sources

To extend this dataset, consider adding:

- **Company Websites**: Product descriptions, about pages
- **Crunchbase/PitchBook**: Detailed funding and valuation data
- **LinkedIn**: Employee counts, growth metrics
- **News/Media**: Coverage, partnerships, announcements
- **Patent Citations**: Technology relationships and influence

Each additional source should:
1. Have its own subdirectory in `/data_collection/`
2. Include extraction scripts
3. Document methodology in README
4. Include data schema/field mapping

## üìÅ Directory Structure

```
data_collection/
‚îú‚îÄ‚îÄ README.md                      # This file - replication guide
‚îú‚îÄ‚îÄ EXTRACT_NOW.html               # Browser-based extraction tool (manual)
‚îú‚îÄ‚îÄ extract_epo_all_methods.py    # ‚≠ê Try all automated methods
‚îú‚îÄ‚îÄ extract_epo_playwright.py     # Playwright with real interactions
‚îú‚îÄ‚îÄ extract_epo_cloudscraper.py   # Cloudscraper bypass method
‚îú‚îÄ‚îÄ extract_epo_automated.py      # Selenium-based (legacy)
‚îú‚îÄ‚îÄ extract_epo_udc.py            # Undetected ChromeDriver (legacy)
‚îî‚îÄ‚îÄ epo_scraper/                   # Legacy scripts
    ‚îú‚îÄ‚îÄ epo_scraper.py
    ‚îú‚îÄ‚îÄ epo_browser_scraper.py
    ‚îî‚îÄ‚îÄ extraction_*.js
```

## üîê Ethics & Legal

- **Purpose**: Academic research only
- **Distribution**: Do not redistribute raw EPO data; share collection methodology instead
- **Citation**: Cite EPO Deep Tech Finder as data source in publications
- **Respect**: Follow EPO's terms of service; do not overload their servers

## üìß Replication Support

If you have issues replicating this data collection:
1. Check this README for troubleshooting
2. Verify you're using latest browser version
3. Ensure EPO website hasn't changed structure
4. Consider using alternative extraction methods in `/epo_scraper/`

---

**Last Updated**: December 2025  
**Extraction Tool Version**: 1.0  
**Expected Yield**: 11,270+ entities

