#!/usr/bin/env python3
"""
Website enrichment for EPO entities (public web).

Goal
----
Collect a reproducible **raw capture** of each company's website text that we can
later compress into "product positioning" variables via LLM prompts.

This script fetches the homepage + (optionally) a small number of internal pages
(1-hop crawl from homepage links) and stores:
- request/response metadata (status, final_url, content_type, timing)
- lightweight page metadata (title, meta description, OG tags, H1, lang)
- extracted visible text (cleaned) suitable for downstream analysis

This script does NOT attempt to bypass anti-bot protections. It records failures
as part of the raw dataset for transparency and reproducibility.

Input:
  research/data/processed/companies.csv  (generated by research/analysis/01_data_processing.py)

Output:
  research/data/enriched/websites_raw_{YYYY-MM-DD}.jsonl
  One JSON object per company with a `pages` array and `combined_text`.

Usage:
  python3 data_collection/enrich_websites.py --limit 50
  python3 data_collection/enrich_websites.py --resume
"""

from __future__ import annotations

import argparse
import hashlib
import json
import random
import re
import signal
import time
from dataclasses import dataclass
from datetime import datetime
from html.parser import HTMLParser
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin, urlparse

import pandas as pd
import requests

CAPTURE_SCHEMA_VERSION = "websites_raw_v1_2026-01-06"
UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"


def strip_html(html: str) -> str:
    # crude but dependency-free: remove scripts/styles then tags
    html = re.sub(r"(?is)<(script|style)[^>]*>.*?</\1>", " ", html or "")
    html = re.sub(r"(?is)<!--.*?-->", " ", html)
    text = re.sub(r"(?is)<[^>]+>", " ", html)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def extract_title(html: str) -> Optional[str]:
    m = re.search(r"(?is)<title[^>]*>(.*?)</title>", html or "")
    if not m:
        return None
    t = re.sub(r"\s+", " ", m.group(1)).strip()
    return t or None


def extract_meta_description(html: str) -> Optional[str]:
    # common forms
    m = re.search(r'(?is)<meta[^>]+name=["\']description["\'][^>]+content=["\']([^"\']*)["\']', html or "")
    if not m:
        m = re.search(r'(?is)<meta[^>]+content=["\']([^"\']*)["\'][^>]+name=["\']description["\']', html or "")
    if not m:
        return None
    d = re.sub(r"\s+", " ", m.group(1)).strip()
    return d or None


def extract_meta_property(html: str, prop: str) -> Optional[str]:
    prop_esc = re.escape(prop)
    m = re.search(rf'(?is)<meta[^>]+property=["\']{prop_esc}["\'][^>]+content=["\']([^"\']*)["\']', html or "")
    if not m:
        m = re.search(rf'(?is)<meta[^>]+content=["\']([^"\']*)["\'][^>]+property=["\']{prop_esc}["\']', html or "")
    if not m:
        return None
    v = re.sub(r"\s+", " ", m.group(1)).strip()
    return v or None


def extract_lang(html: str) -> Optional[str]:
    m = re.search(r'(?is)<html[^>]+lang=["\']([a-zA-Z-]{2,15})["\']', html or "")
    if not m:
        return None
    return (m.group(1) or "").strip() or None


def extract_h1(html: str) -> Optional[str]:
    m = re.search(r"(?is)<h1[^>]*>(.*?)</h1>", html or "")
    if not m:
        return None
    h = re.sub(r"(?is)<[^>]+>", " ", m.group(1))
    h = re.sub(r"\s+", " ", h).strip()
    return h or None


def sha256_text(s: str) -> str:
    return hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest()


def norm_domain(u: str) -> Optional[str]:
    if not u or not isinstance(u, str):
        return None
    u = u.strip()
    if not u:
        return None
    if not re.match(r"^https?://", u, flags=re.IGNORECASE):
        u = "https://" + u
    try:
        host = urlparse(u).netloc.lower()
        host = host[4:] if host.startswith("www.") else host
        return host or None
    except Exception:
        return None


def is_http_url(u: str) -> bool:
    try:
        p = urlparse(u)
        return p.scheme in {"http", "https"} and bool(p.netloc)
    except Exception:
        return False


class LinkParser(HTMLParser):
    """Extract <a href> links and their visible anchor text (rough)."""

    def __init__(self):
        super().__init__()
        self.links: List[Dict[str, str]] = []
        self._in_a = False
        self._cur_href: Optional[str] = None
        self._cur_text_parts: List[str] = []

    def handle_starttag(self, tag, attrs):
        if tag.lower() != "a":
            return
        href = None
        for k, v in attrs:
            if k.lower() == "href":
                href = v
                break
        if href:
            self._in_a = True
            self._cur_href = href
            self._cur_text_parts = []

    def handle_data(self, data):
        if self._in_a and data:
            self._cur_text_parts.append(data)

    def handle_endtag(self, tag):
        if tag.lower() != "a":
            return
        if self._in_a and self._cur_href:
            text = re.sub(r"\s+", " ", "".join(self._cur_text_parts)).strip()
            self.links.append({"href": self._cur_href, "text": text})
        self._in_a = False
        self._cur_href = None
        self._cur_text_parts = []


@dataclass
class FetchResult:
    ok: bool
    http_status: Optional[int]
    final_url: Optional[str]
    content_type: Optional[str]
    elapsed_ms: Optional[int]
    html: str
    error: Optional[str]
    attempts: int
    encoding: Optional[str]
    encoding_source: Optional[str]


def extract_charset_from_content_type(ct: Optional[str]) -> Optional[str]:
    if not ct or not isinstance(ct, str):
        return None
    m = re.search(r"(?i)\bcharset\s*=\s*([^\s;]+)", ct)
    if not m:
        return None
    cs = m.group(1).strip().strip('"').strip("'").strip()
    return cs.lower() or None


def extract_charset_from_meta(raw: bytes) -> Optional[str]:
    # Only inspect a small prefix; most sites put meta charset in <head>.
    try:
        head = raw[:8192].decode("ascii", errors="ignore")
    except Exception:
        return None

    # <meta charset="utf-8">
    m = re.search(r'(?is)<meta[^>]+charset\s*=\s*["\']?\s*([a-zA-Z0-9_\-]+)\s*["\']?', head)
    if m:
        return (m.group(1) or "").strip().lower() or None

    # <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    m = re.search(
        r'(?is)<meta[^>]+http-equiv\s*=\s*["\']content-type["\'][^>]+content\s*=\s*["\'][^"\']*charset\s*=\s*([a-zA-Z0-9_\-]+)[^"\']*["\']',
        head,
    )
    if m:
        return (m.group(1) or "").strip().lower() or None

    return None


def normalize_charset(cs: Optional[str]) -> Optional[str]:
    if not cs:
        return None
    cs = cs.strip().lower()
    # common aliases
    aliases = {
        "utf8": "utf-8",
        "utf_8": "utf-8",
        "latin1": "latin-1",
        "latin-1": "latin-1",
        "iso8859-1": "iso-8859-1",
        "windows1252": "windows-1252",
        "cp1252": "windows-1252",
    }
    return aliases.get(cs, cs)


def looks_like_utf8(raw: bytes) -> bool:
    """
    Return True if bytes are valid UTF-8, allowing a truncated final multibyte
    sequence (common when we cap max_bytes).
    """
    try:
        raw.decode("utf-8", errors="strict")
        return True
    except UnicodeDecodeError as e:
        # tolerate invalid sequence only if it starts within the last 4 bytes
        if e.start >= max(0, len(raw) - 4):
            try:
                raw[: e.start].decode("utf-8", errors="strict")
                return True
            except Exception:
                return False
        return False


def decode_html_bytes(raw: bytes, content_type: Optional[str], apparent: Optional[str]) -> tuple[str, str, str]:
    """
    Decode HTML bytes to str with improved charset handling.

    Priority:
    1) meta charset (authoritative)
    2) if bytes are valid UTF-8 -> utf-8
    3) header charset
    4) requests apparent_encoding
    5) utf-8 fallback

    Returns: (decoded_text, decoded_encoding, encoding_source)
    """
    meta_cs = normalize_charset(extract_charset_from_meta(raw))
    if meta_cs:
        enc = meta_cs
        src = "meta"
    elif looks_like_utf8(raw):
        enc = "utf-8"
        src = "utf8_strict"
    else:
        header_cs = normalize_charset(extract_charset_from_content_type(content_type))
        if header_cs:
            enc = header_cs
            src = "header"
        else:
            app_cs = normalize_charset(apparent)
            if app_cs:
                enc = app_cs
                src = "apparent"
            else:
                enc = "utf-8"
                src = "fallback"

    try:
        text = raw.decode(enc, errors="replace")
    except Exception:
        # last resort
        text = raw.decode("utf-8", errors="replace")
        enc = "utf-8"
        src = "fallback"

    return text, enc, src


def detect_apparent_charset(raw: bytes) -> Optional[str]:
    """
    Best-effort charset detection from raw bytes without relying on Response.content.

    We avoid using `requests.Response.apparent_encoding` because when we stream the body
    (`stream=True`) and manually consume it, accessing `.apparent_encoding` can raise:
    "The content for this response was already consumed".
    """
    if not raw:
        return None
    # Prefer charset_normalizer (requests' default on many installs)
    try:
        from charset_normalizer import from_bytes  # type: ignore

        best = from_bytes(raw).best()
        enc = getattr(best, "encoding", None) if best is not None else None
        if enc:
            return str(enc)
    except Exception:
        pass
    # Fallback to chardet if present
    try:
        import chardet  # type: ignore

        det = chardet.detect(raw)
        enc = det.get("encoding") if isinstance(det, dict) else None
        if enc:
            return str(enc)
    except Exception:
        pass
    return None


def fetch_html(
    session: requests.Session,
    url: str,
    *,
    timeout: int,
    max_bytes: int,
    max_attempts: int,
    backoff_base: float,
    backoff_max: float,
    jitter: float,
) -> FetchResult:
    """
    Fetch HTML with retry/backoff.

    `ok` means: we received a response and it was not an obvious error status (<400).
    We still return the (possibly error) HTML for transparency.
    """
    retry_statuses = {408, 425, 429, 500, 502, 503, 504, 522, 523, 524}
    last_err: Optional[str] = None
    last_resp: Optional[requests.Response] = None
    last_html: str = ""
    last_elapsed_ms: Optional[int] = None

    max_attempts = max(1, int(max_attempts))

    for attempt in range(1, max_attempts + 1):
        try:
            t0 = time.monotonic()

            # Guard against rare hangs that can bypass requests' timeout (e.g., DNS resolution).
            alarm_seconds = max(5.0, float(timeout) + 5.0)
            old_handler = None
            old_itimer = None

            def _alarm_handler(signum, frame):  # type: ignore[no-untyped-def]
                raise TimeoutError(f"Hard timeout after {alarm_seconds:.1f}s")

            try:
                if hasattr(signal, "SIGALRM"):
                    old_handler = signal.signal(signal.SIGALRM, _alarm_handler)
                    # setitimer supports fractional seconds; SIGALRM is process-wide
                    old_itimer = signal.setitimer(signal.ITIMER_REAL, alarm_seconds)
                # Stream to avoid hanging on very large or slow responses; we only keep up to max_bytes.
                r = session.get(url, timeout=timeout, allow_redirects=True, stream=True)
            finally:
                if hasattr(signal, "SIGALRM"):
                    try:
                        signal.setitimer(signal.ITIMER_REAL, 0.0)
                    except Exception:
                        pass
                    if old_handler is not None:
                        try:
                            signal.signal(signal.SIGALRM, old_handler)
                        except Exception:
                            pass
            last_resp = r
            # Read at most max_bytes from the body.
            buf = bytearray()
            try:
                for chunk in r.iter_content(chunk_size=16384):
                    if not chunk:
                        continue
                    buf.extend(chunk)
                    if max_bytes and len(buf) >= max_bytes:
                        break
                    # hard cap total wall time for body download
                    if (time.monotonic() - t0) > alarm_seconds:
                        raise TimeoutError(f"Body download hard timeout after {alarm_seconds:.1f}s")
            finally:
                try:
                    r.close()
                except Exception:
                    pass

            last_elapsed_ms = int((time.monotonic() - t0) * 1000)

            raw = bytes(buf)
            if max_bytes and len(raw) > max_bytes:
                raw = raw[:max_bytes]
            apparent = detect_apparent_charset(raw)
            html, enc_used, enc_src = decode_html_bytes(raw, r.headers.get("content-type"), apparent)
            last_html = html or ""

            status = int(r.status_code)
            ok = status < 400
            err = None if ok else f"HTTP {status}"

            # Retry only for transient statuses
            if status in retry_statuses and attempt < max_attempts:
                # Respect Retry-After when provided
                ra = r.headers.get("retry-after")
                sleep_s: Optional[float] = None
                if ra:
                    try:
                        sleep_s = float(ra)
                    except Exception:
                        sleep_s = None
                if sleep_s is None:
                    sleep_s = min(float(backoff_max), float(backoff_base) * (2 ** (attempt - 1)))
                sleep_s = max(0.0, sleep_s) + (random.uniform(0.0, float(jitter)) if jitter else 0.0)
                time.sleep(sleep_s)
                continue

            return FetchResult(
                ok=ok,
                http_status=status,
                final_url=str(r.url),
                content_type=r.headers.get("content-type"),
                elapsed_ms=last_elapsed_ms,
                html=last_html,
                error=err,
                attempts=attempt,
                encoding=enc_used,
                encoding_source=enc_src,
            )
        except Exception as e:
            last_err = str(e)
            if attempt < max_attempts:
                sleep_s = min(float(backoff_max), float(backoff_base) * (2 ** (attempt - 1)))
                sleep_s = max(0.0, sleep_s) + (random.uniform(0.0, float(jitter)) if jitter else 0.0)
                time.sleep(sleep_s)
                continue
            return FetchResult(
                ok=False,
                http_status=None,
                final_url=None,
                content_type=None,
                elapsed_ms=None,
                html="",
                error=last_err,
                attempts=attempt,
                encoding=None,
                encoding_source=None,
            )

    # unreachable, but keep mypy happy
    return FetchResult(
        ok=False,
        http_status=int(last_resp.status_code) if last_resp is not None else None,
        final_url=str(last_resp.url) if last_resp is not None else None,
        content_type=last_resp.headers.get("content-type") if last_resp is not None else None,
        elapsed_ms=last_elapsed_ms,
        html=last_html,
        error=last_err or "Fetch failed",
        attempts=max_attempts,
        encoding=normalize_charset(getattr(last_resp, "encoding", None)) if last_resp is not None else None,
        encoding_source=None,
    )


def score_link(url: str, anchor_text: str) -> int:
    u = (url or "").lower()
    t = (anchor_text or "").lower()
    score = 0

    if len(u) < 120:
        score += 1

    patterns = {
        "about": 6,
        "company": 4,
        "who-we-are": 5,
        "mission": 4,
        "story": 3,
        "product": 6,
        "products": 6,
        "platform": 5,
        "solution": 6,
        "solutions": 6,
        "use-case": 5,
        "usecases": 5,
        "technology": 6,
        "tech": 4,
        "science": 3,
        "industr": 4,
        "customers": 3,
        "case": 3,
        "pricing": 2,
    }
    for k, w in patterns.items():
        if k in u:
            score += w
        if k in t:
            score += max(1, w - 2)
    return score


def pick_internal_pages(home_final_url: str, home_html: str, max_pages: int) -> List[Dict[str, str]]:
    """
    Select up to (max_pages-1) internal page URLs to fetch, based on homepage links.
    Returns list of {"kind": "...", "url": "..."}.
    """
    if max_pages <= 1 or not home_final_url:
        return []

    base_domain = norm_domain(home_final_url)
    if not base_domain:
        return []

    parser = LinkParser()
    try:
        parser.feed(home_html or "")
    except Exception:
        return []

    candidates: List[Dict[str, Any]] = []
    seen = set()
    for link in parser.links:
        href = (link.get("href") or "").strip()
        if not href or href.startswith(("mailto:", "tel:", "javascript:", "#")):
            continue

        abs_url = urljoin(home_final_url, href)
        abs_url = abs_url.split("#")[0]
        if not is_http_url(abs_url):
            continue

        d = norm_domain(abs_url)
        if not d or d != base_domain:
            continue

        if re.search(r"(?i)\.(pdf|png|jpg|jpeg|gif|svg|zip)$", abs_url):
            continue

        # Avoid obvious non-positioning pages / traps
        if re.search(
            r"(?i)(/wp-admin|/wp-json|/login|/signin|/signup|/account|/privacy|/terms|cookie|gdpr|impressum|/legal|/feed|/rss)",
            abs_url,
        ):
            continue

        if abs_url in seen:
            continue
        seen.add(abs_url)

        s = score_link(abs_url, link.get("text") or "")
        if s <= 0:
            continue

        candidates.append({"url": abs_url, "score": s})

    candidates.sort(key=lambda x: (-x["score"], len(x["url"])))

    picked: List[Dict[str, str]] = []
    for c in candidates:
        u = c["url"].lower()
        kind = "other"
        if "about" in u or "who-we-are" in u or "company" in u:
            kind = "about"
        elif "product" in u or "products" in u or "platform" in u:
            kind = "product"
        elif "solution" in u or "use-case" in u or "usecases" in u:
            kind = "solutions"
        elif "technology" in u or "/tech" in u:
            kind = "technology"
        elif "pricing" in u:
            kind = "pricing"

        if any(p["kind"] == kind for p in picked):
            continue
        picked.append({"kind": kind, "url": c["url"]})
        if len(picked) >= max_pages - 1:
            break

    return picked


def read_done_company_ids(jsonl_path: Path) -> set:
    done = set()
    if not jsonl_path.exists():
        return done
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            cid = obj.get("company_id") or obj.get("id")
            if cid:
                done.add(str(cid))
    return done


def shard_for_company_id(company_id: str, shard_total: int) -> int:
    """
    Deterministic sharding for batching/parallelism.
    Uses SHA256(company_id) mod shard_total.
    """
    shard_total = max(1, int(shard_total))
    h = hashlib.sha256(company_id.encode("utf-8", errors="ignore")).hexdigest()
    return int(h, 16) % shard_total


def throttle_domain(domain: Optional[str], last_by_domain: Dict[str, float], domain_delay: float, jitter: float) -> None:
    """
    Ensure at least `domain_delay` seconds between requests to the same domain.
    """
    if not domain or domain_delay <= 0:
        return
    now = time.monotonic()
    last = last_by_domain.get(domain)
    if last is not None:
        wait = float(domain_delay) - (now - last)
        if wait > 0:
            time.sleep(wait + (random.uniform(0.0, float(jitter)) if jitter else 0.0))
    last_by_domain[domain] = time.monotonic()


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--limit", type=int, default=0, help="Max number of companies to write in this run (batching)")
    ap.add_argument("--sleep", type=float, default=0.5, help="Base delay between HTTP requests (seconds)")
    ap.add_argument("--domain-delay", type=float, default=1.0, help="Min delay between requests to the same domain (seconds)")
    ap.add_argument("--jitter", type=float, default=0.2, help="Random jitter added to sleeps/backoff (seconds)")
    ap.add_argument("--timeout", type=int, default=20, help="HTTP request timeout (seconds)")
    ap.add_argument("--retries", type=int, default=3, help="Max attempts per page fetch")
    ap.add_argument("--backoff-base", type=float, default=1.0, help="Backoff base seconds for retries")
    ap.add_argument("--backoff-max", type=float, default=20.0, help="Backoff max seconds for retries")
    ap.add_argument("--max-bytes", type=int, default=2_000_000, help="Max bytes to keep per page response")
    ap.add_argument("--max-pages", type=int, default=3, help="Max pages per company (homepage + internal pages)")
    ap.add_argument("--max-text-chars", type=int, default=60000, help="Max extracted text chars per page")
    ap.add_argument("--max-combined-chars", type=int, default=180000, help="Max combined text chars per company")
    ap.add_argument("--resume", action="store_true", help="Skip company_ids already present in output")
    ap.add_argument("--output", default="", help="Override output path")
    ap.add_argument("--company-ids-file", default="", help="Optional newline-delimited company_id list to restrict this run")
    ap.add_argument(
        "--company-ids-order",
        default="sorted",
        choices=["sorted", "file", "sha256"],
        help="When --company-ids-file is used, choose iteration order: sorted (default), file order, or sha256(id) order.",
    )
    ap.add_argument("--shard-index", type=int, default=0, help="Shard index (0-based) for parallel/batched runs")
    ap.add_argument("--shard-total", type=int, default=1, help="Total shards for parallel/batched runs")
    ap.add_argument(
        "--max-runtime-minutes",
        type=float,
        default=0.0,
        help="Stop after N minutes (0 = no limit). Useful for safe batching.",
    )
    args = ap.parse_args()

    inp = Path("research/data/processed/companies.csv")
    if not inp.exists():
        raise SystemExit("Missing research/data/processed/companies.csv. Run: python3 research/analysis/01_data_processing.py")

    df = pd.read_csv(inp)
    urls = df[["id", "name", "homepage_url", "homepage_url_raw"]].copy()
    urls["url"] = urls["homepage_url"].fillna(urls["homepage_url_raw"])
    urls = urls[urls["url"].notna()]
    # Stable order for batching
    urls = urls.sort_values(by=["id", "name"], kind="mergesort")

    shard_total = max(1, int(args.shard_total))
    shard_index = int(args.shard_index)
    if shard_index < 0 or shard_index >= shard_total:
        raise SystemExit(f"Invalid shard: --shard-index must be in [0, {shard_total - 1}]")

    if shard_total > 1:
        urls = urls[urls["id"].astype(str).map(lambda cid: shard_for_company_id(cid, shard_total) == shard_index)]

    if args.company_ids_file:
        ids_path = Path(str(args.company_ids_file)).expanduser()
        if not ids_path.exists():
            raise SystemExit(f"Missing --company-ids-file: {ids_path}")
        wanted_list: List[str] = []
        wanted: set = set()
        with open(ids_path, "r", encoding="utf-8") as f:
            for line in f:
                line = (line or "").strip()
                if not line:
                    continue
                cid = str(line)
                wanted.add(cid)
                wanted_list.append(cid)
        before = len(urls)
        urls = urls[urls["id"].astype(str).isin(wanted)]
        print(f"Restricted companies: {len(urls)}/{before} from {ids_path}")
        # Override iteration order for restricted runs if requested.
        if args.company_ids_order == "file":
            rank = {cid: i for i, cid in enumerate(wanted_list)}
            urls["_order_rank"] = urls["id"].astype(str).map(rank)
            urls = urls.sort_values(by=["_order_rank"], kind="mergesort").drop(columns=["_order_rank"])
        elif args.company_ids_order == "sha256":
            urls["_order_rank"] = urls["id"].astype(str).map(
                lambda cid: hashlib.sha256(cid.encode("utf-8", errors="ignore")).hexdigest()
            )
            urls = urls.sort_values(by=["_order_rank"], kind="mergesort").drop(columns=["_order_rank"])

    out_dir = Path("research/data/enriched")
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = Path(args.output) if args.output else (out_dir / f"websites_raw_{datetime.utcnow().strftime('%Y-%m-%d')}.jsonl")

    done_ids = read_done_company_ids(out_path) if args.resume else set()
    if done_ids:
        print(f"Resume mode: skipping {len(done_ids)} already-done companies")

    mode = "a" if args.resume and out_path.exists() else "w"
    n = len(urls)
    if shard_total > 1:
        print(f"Shard: {shard_index}/{shard_total} (deterministic by company_id hash)")
    print(f"Eligible websites in this run: {n} -> {out_path}")
    if args.limit and args.limit > 0:
        print(f"Batch limit: will write up to {int(args.limit)} companies then stop.")

    session = requests.Session()
    session.headers.update({"User-Agent": UA, "Accept": "text/html,application/xhtml+xml"})

    last_by_domain: Dict[str, float] = {}
    t_start = time.monotonic()
    # Cleanup old domain entries periodically to prevent unbounded growth
    domain_cleanup_interval = 1000  # Clean every 1000 companies
    domain_cleanup_age_sec = 3600  # Remove entries older than 1 hour

    with open(out_path, mode, encoding="utf-8") as f:
        written = 0
        ok_companies = 0
        for i, row in enumerate(urls.itertuples(index=False), start=1):
            # Periodic cleanup of old domain throttle entries
            if i % domain_cleanup_interval == 0:
                now = time.monotonic()
                expired = [d for d, t in last_by_domain.items() if (now - t) > domain_cleanup_age_sec]
                for d in expired:
                    del last_by_domain[d]
                if expired:
                    print(f"  Cleaned {len(expired)} old domain throttle entries")
            if args.max_runtime_minutes and args.max_runtime_minutes > 0:
                if (time.monotonic() - t_start) >= float(args.max_runtime_minutes) * 60.0:
                    print(f"⏱️  Reached max runtime ({args.max_runtime_minutes} minutes). Stopping cleanly.")
                    break

            if args.limit and args.limit > 0 and written >= int(args.limit):
                break

            company_id = str(row.id)
            if company_id in done_ids:
                continue

            source_url = str(row.url).strip()
            fetched_at = datetime.utcnow().isoformat() + "Z"

            pages: List[Dict[str, Any]] = []
            combined_text_parts: List[str] = []

            # Homepage
            throttle_domain(norm_domain(source_url), last_by_domain, args.domain_delay, args.jitter)
            home = fetch_html(
                session,
                source_url,
                timeout=args.timeout,
                max_bytes=args.max_bytes,
                max_attempts=args.retries,
                backoff_base=args.backoff_base,
                backoff_max=args.backoff_max,
                jitter=args.jitter,
            )
            home_text = strip_html(home.html)[: max(0, args.max_text_chars)] if home.ok else ""

            pages.append(
                {
                    "kind": "homepage",
                    "source_url": source_url,
                    "ok": home.ok,
                    "http_status": home.http_status,
                    "final_url": home.final_url,
                    "final_domain": norm_domain(home.final_url) if home.final_url else None,
                    "content_type": home.content_type,
                    "elapsed_ms": home.elapsed_ms,
                    "bytes": len(home.html.encode("utf-8", errors="ignore")) if home.html else 0,
                    "attempts": home.attempts,
                    "encoding": home.encoding,
                    "encoding_source": home.encoding_source,
                    "title": extract_title(home.html) if home.ok else None,
                    "meta_description": extract_meta_description(home.html) if home.ok else None,
                    "og_title": extract_meta_property(home.html, "og:title") if home.ok else None,
                    "og_description": extract_meta_property(home.html, "og:description") if home.ok else None,
                    "h1": extract_h1(home.html) if home.ok else None,
                    "lang": extract_lang(home.html) if home.ok else None,
                    "text": home_text,
                    "text_char_count": len(home_text),
                    "text_sha256": sha256_text(home_text) if home_text else None,
                    "html_sha256": sha256_text(home.html) if home.html else None,
                    "error": home.error if not home.ok else None,
                }
            )
            if home_text:
                combined_text_parts.append(f"[homepage]\\n{home_text}")

            internal_pages = (
                pick_internal_pages(home.final_url, home.html, max_pages=max(1, int(args.max_pages))) if home.ok and home.final_url else []
            )

            visited = {home.final_url or source_url}
            for p in internal_pages:
                u = p["url"]
                if u in visited:
                    continue
                visited.add(u)

                throttle_domain(norm_domain(u), last_by_domain, args.domain_delay, args.jitter)
                fr = fetch_html(
                    session,
                    u,
                    timeout=args.timeout,
                    max_bytes=args.max_bytes,
                    max_attempts=args.retries,
                    backoff_base=args.backoff_base,
                    backoff_max=args.backoff_max,
                    jitter=args.jitter,
                )
                txt = strip_html(fr.html)[: max(0, args.max_text_chars)] if fr.ok else ""
                pages.append(
                    {
                        "kind": p["kind"],
                        "source_url": u,
                        "ok": fr.ok,
                        "http_status": fr.http_status,
                        "final_url": fr.final_url,
                        "final_domain": norm_domain(fr.final_url) if fr.final_url else None,
                        "content_type": fr.content_type,
                        "elapsed_ms": fr.elapsed_ms,
                        "bytes": len(fr.html.encode("utf-8", errors="ignore")) if fr.html else 0,
                        "attempts": fr.attempts,
                        "encoding": fr.encoding,
                        "encoding_source": fr.encoding_source,
                        "title": extract_title(fr.html) if fr.ok else None,
                        "meta_description": extract_meta_description(fr.html) if fr.ok else None,
                        "og_title": extract_meta_property(fr.html, "og:title") if fr.ok else None,
                        "og_description": extract_meta_property(fr.html, "og:description") if fr.ok else None,
                        "h1": extract_h1(fr.html) if fr.ok else None,
                        "lang": extract_lang(fr.html) if fr.ok else None,
                        "text": txt,
                        "text_char_count": len(txt),
                        "text_sha256": sha256_text(txt) if txt else None,
                        "html_sha256": sha256_text(fr.html) if fr.html else None,
                        "error": fr.error if not fr.ok else None,
                    }
                )
                if txt:
                    combined_text_parts.append(f"[{p['kind']}]\\n{txt}")

                time.sleep(max(0.0, args.sleep))

            combined_text = "\n\n".join(combined_text_parts)
            if args.max_combined_chars and args.max_combined_chars > 0:
                combined_text = combined_text[: int(args.max_combined_chars)]

            rec = {
                "capture_schema_version": CAPTURE_SCHEMA_VERSION,
                "company_id": company_id,
                "company_name": row.name,
                "homepage_url_input": source_url,
                "fetched_at_utc": fetched_at,
                "user_agent": UA,
                "max_pages": int(args.max_pages),
                "shard_index": shard_index,
                "shard_total": shard_total,
                "pages": pages,
                "combined_text": combined_text,
                "combined_text_char_count": len(combined_text),
                "combined_text_sha256": sha256_text(combined_text) if combined_text else None,
                "ok": any(p.get("ok") for p in pages),
            }

            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            written += 1
            if rec["ok"]:
                ok_companies += 1

            if written % 25 == 0 or i == n or (args.limit and args.limit > 0 and written >= int(args.limit)):
                print(f"  wrote {written} records (scanned {i}/{n})")

            time.sleep(max(0.0, args.sleep))

        print(f"✅ Done. Wrote {written} companies ({ok_companies} with >=1 successful page).")


if __name__ == "__main__":
    main()